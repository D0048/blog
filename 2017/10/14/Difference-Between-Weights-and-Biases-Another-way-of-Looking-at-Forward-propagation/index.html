<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>Difference Between Weights and Biases: Another way of Looking at Forward Propagation | D0048</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="================LATEX MATH TEST================:$$\begin{eqnarray}\nabla\times\vec{B} &amp;amp;=&amp;amp; \mu_0\left(\vec{J}+\epsilon_0\frac{\partial E}{\partial t} \right)\end{eqnarray}$$IF THE ABOVE IS NOT">
<meta name="keywords" content="Machine Learning">
<meta property="og:type" content="article">
<meta property="og:title" content="Difference Between Weights and Biases: Another way of Looking at Forward Propagation">
<meta property="og:url" content="https://d0048.github.io/blog/2017/10/14/Difference-Between-Weights-and-Biases-Another-way-of-Looking-at-Forward-propagation/index.html">
<meta property="og:site_name" content="D0048">
<meta property="og:description" content="================LATEX MATH TEST================:$$\begin{eqnarray}\nabla\times\vec{B} &amp;amp;=&amp;amp; \mu_0\left(\vec{J}+\epsilon_0\frac{\partial E}{\partial t} \right)\end{eqnarray}$$IF THE ABOVE IS NOT">
<meta property="og:locale" content="en">
<meta property="og:updated_time" content="2017-10-15T12:08:19.805Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Difference Between Weights and Biases: Another way of Looking at Forward Propagation">
<meta name="twitter:description" content="================LATEX MATH TEST================:$$\begin{eqnarray}\nabla\times\vec{B} &amp;amp;=&amp;amp; \mu_0\left(\vec{J}+\epsilon_0\frac{\partial E}{\partial t} \right)\end{eqnarray}$$IF THE ABOVE IS NOT">
  
    <link rel="alternate" href="/blog/atom.xml" title="D0048" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/blog/css/style.css">
  
<!-- Google Analytics -->
<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-88350074-1', 'auto');
ga('send', 'pageview');

</script>
<!-- End Google Analytics --><!-- hexo-inject:begin --><!-- hexo-inject:end -->


</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/blog/" id="logo">D0048</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/blog/" id="subtitle">A record of thoughts, ideas, solutions and traps</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/blog/">Home</a>
        
          <a class="main-nav-link" href="/blog/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/blog/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://d0048.github.io/blog"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-Difference-Between-Weights-and-Biases-Another-way-of-Looking-at-Forward-propagation" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/blog/2017/10/14/Difference-Between-Weights-and-Biases-Another-way-of-Looking-at-Forward-propagation/" class="article-date">
  <time datetime="2017-10-14T11:53:39.000Z" itemprop="datePublished">2017-10-14</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Difference Between Weights and Biases: Another way of Looking at Forward Propagation
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>================LATEX MATH TEST================:<br>$$<br>\begin{eqnarray}<br>\nabla\times\vec{B} &amp;=&amp; \mu_0\left(\vec{J}+\epsilon_0\frac{\partial E}{\partial t} \right)<br>\end{eqnarray}<br>$$<br><em>IF THE ABOVE IS NOT SHOWING PROPERLY, PLEASE UNBLOCK UNSECURE(HTTP) CROSS SITE SCRIPTS IN YOUR BROWSER</em><br>================LATEX MATH TEST================:</p>
<h2 id="What-are-Weights-and-Biases"><a href="#What-are-Weights-and-Biases" class="headerlink" title="What are Weights and Biases"></a>What are Weights and Biases</h2><p>Consider the following forward propagation algorithm:<br>$$<br>\vec{y_{n}}=\mathbf{W_n}^T \times  \vec{y_{n-1}} + \vec{b_n}<br>$$<br>where $n$ is the number of the layers, $\vec{y_n}$ is the output of the $n^{th}$ layer, expressed as a $l_n \times 1$ ($l_n$ is the number of neurons of the $n^th$ layer) vector. $\mathbf{W_n}$ is a $l_{n-1} \times l_{n}$ matrix storing all the weights of every connection between layer $n$ and $n-1$, thus needing to be transposed for the sake of the product. $\vec{b_n}$, again, is the biases of the connections between the $n^th$ and $(n-1)^th$ layers, in the shape of $l_n\times1$.</p>
<p>As one can see, both weights and biases are just changeable and derivable(thus trainable) factors that contributes to the final results.</p>
<h2 id="Why-do-we-need-both-of-them-and-why-are-Biases-Optional"><a href="#Why-do-we-need-both-of-them-and-why-are-Biases-Optional" class="headerlink" title="Why do we need both of them, and why are Biases Optional?"></a>Why do we need both of them, and why are Biases Optional?</h2><p>Neural network, indeed a better version of the perceptron model, where the output of each neuron(perceptron) owns a linear correlation with the output, rather than simply outputting plain 0/1. (This relation is further more projected to the activation function to make it non-linear, which will be discussed later) </p>
<p>To create a linear correlation, the easiest way is to scale the input with a certain coefficient $w$, output the scaled input.<br>$$<br>f(x)=w\times x<br>$$</p>
<p>This model works alright, even with one neuron it could perfectly fit a linear function like $f(x)=m\times x$, and certain non-linear relations could be fit with neurons work in layers. </p>
<p>However, this new neuron without biases, lack of a significant ability even comparing to perceptron: it always fires regardless the input thus failing to fit functions like $y=mx+b$. It’s impossible to disable the output of a specific neuron on certain threshold value of the input. Even that adding more layers and neurons a lot eases and hides this issue, neural networks without biases are likely to perform a worse job than those with biases.(Consider the total layers/neurons are the same)</p>
<p>In conclusion, the biases are supplements to the weights to help a network better fit the pattern, which are not necessary but helps the network to perform better. </p>
<h2 id="Another-way-of-writing-the-Forward-Propagation"><a href="#Another-way-of-writing-the-Forward-Propagation" class="headerlink" title="Another way of writing the Forward Propagation"></a>Another way of writing the Forward Propagation</h2><p>Interestingly, the forward propagation algorithm<br>$$<br>\vec{y_{n}}=\mathbf{W_n}^T \times  \vec{y_{n-1}} + 1 \times \vec{b_n}<br>$$<br>could also be written like this:<br>$$<br>\vec{y_{n}}=<br>\left[ \begin{array}{c}<br>                x, \ 1<br>\end{array} \right]^T<br>\cdot<br>\left[ \begin{array}{c}<br>                \mathbf{W_n},<br>                \ \vec{b_n}<br>\end{array} \right]<br>$$,which is<br>$$<br>\vec{y_{n}} = \vec{y_{new_{n-1}}}^T \times \vec{W_{new}}<br>$$.<br>This is a way of rewriting the equation makes the adjustment by gradient really easy to write.</p>
<h2 id="How-to-update-them"><a href="#How-to-update-them" class="headerlink" title="How to update them?"></a>How to update them?</h2><p>It’s super easy after the rewrite:<br>$$<br>\vec{W_{new}} =\vec{W_{new}}-\frac{\delta W_{new}}{\delta Error}<br>$$.</p>
<h2 id="The-Activation-Function"><a href="#The-Activation-Function" class="headerlink" title="The Activation Function"></a>The Activation Function</h2><p>There is one more compoment yet to be mentioned–the Activation Function. It’s basically a function takes the output of a neuron as an input and output whatever value defined as the final output of the neuron.<br>$$<br>\vec{W_{new}} =Activation(\vec{W_{new}}-\frac{\delta W_{new}}{\delta Error})<br>$$<br>There are copious types of them around, but all of them have at least one shared property that there are all <em>Non-linear</em>! </p>
<p>That’s basically what they are designed for. Activation Functions project output to a non-linear function, thus introducing non-linearity into the model. </p>
<p>Consider non-linear-seperatable problems like the the XOR problem, giving the network the ability to draw non-linear sperators may help the classification.</p>
<p>Also, there’s another purpose of the activation function, which is to project a huge input, into the space between -1 and 1, thus making the followed-up calculations easier and faster.<br><br><br>2017/10/15</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://d0048.github.io/blog/2017/10/14/Difference-Between-Weights-and-Biases-Another-way-of-Looking-at-Forward-propagation/" data-id="cj8sie5u5000s26akbfoymvrq" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/Machine-Learning/">Machine Learning</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/blog/2017/10/12/Pass-Strings-from-Python-to-C-CPP-libs/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Pass Strings from Python to C/CPP libs</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/Machine-Learning/">Machine Learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/Python/">Python</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/blog/tags/Machine-Learning/" style="font-size: 20px;">Machine Learning</a> <a href="/blog/tags/Python/" style="font-size: 10px;">Python</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2017/10/">October 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2017/09/">September 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2017/08/">August 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2017/07/">July 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2017/06/">June 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2017/05/">May 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2017/04/">April 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2017/03/">March 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2017/02/">February 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2017/01/">January 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2016/12/">December 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2016/11/">November 2016</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/blog/2017/10/14/Difference-Between-Weights-and-Biases-Another-way-of-Looking-at-Forward-propagation/">Difference Between Weights and Biases: Another way of Looking at Forward Propagation</a>
          </li>
        
          <li>
            <a href="/blog/2017/10/12/Pass-Strings-from-Python-to-C-CPP-libs/">Pass Strings from Python to C/CPP libs</a>
          </li>
        
          <li>
            <a href="/blog/2017/10/05/Some-Thoughts-on-Deep-Neural-Networks-and-Handwritten-digit-recognition/">Some Thoughts on Deep Neural Networks and Handwritten digit recognition</a>
          </li>
        
          <li>
            <a href="/blog/2017/09/24/update-pepper-flash-player-under-linux/">Update pepper flash player under linux</a>
          </li>
        
          <li>
            <a href="/blog/2017/09/12/convert-an-image-into-an-xbitmap-array/">Convert an image into an XBitMap C array stored in a C header file</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">

<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://d0048.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
                      
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2017 D0048<br>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/blog/" class="mobile-nav-link">Home</a>
  
    <a href="/blog/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/blog/fancybox/jquery.fancybox.css">
  <script src="/blog/fancybox/jquery.fancybox.pack.js"></script>


<script src="/blog/js/script.js"></script>

  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>